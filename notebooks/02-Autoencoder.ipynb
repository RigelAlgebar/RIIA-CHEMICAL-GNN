{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from typing import Any, List, Tuple, Dict\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Implementation of the reparameterization trick.\n",
    "    \n",
    "    Sampling...\n",
    "    \n",
    "    \n",
    "    Attributes:\n",
    "        inputs: Tensor.\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        A tensor of samples obtained from the latent space. \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "\n",
    "    def call(self, inputs: Tuple) -> tf.Tensor:\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch   = tf.shape(z_mean)[0]\n",
    "        dim     = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Encoder Model.\n",
    "    \n",
    "    The encoder contains the model that projects the input into the laten space,\n",
    "    as well as regression model.\n",
    "    \"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "\n",
    "    def __init__(self, original_dim, latent_dim=2, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.original_dim  = original_dim\n",
    "        self.latent_dim    = latent_dim\n",
    "        self.sampling      = Sampling()\n",
    "        self.dense_mean    = layers.Dense(latent_dim, activation='relu')\n",
    "        self.dense_log_var = layers.Dense(latent_dim, activation='relu')\n",
    "        self.reg_mean      = layers.Dense(1)\n",
    "        self.reg_log_var   = layers.Dense(1)\n",
    "        self.projection    = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(512, activation='relu', input_shape=(self.original_dim,)),\n",
    "                tf.keras.layers.Dense(256, activation='relu' ),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        x         = self.projection(inputs)\n",
    "        z_mean    = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z         = self.sampling((z_mean, z_log_var))\n",
    "\n",
    "        return z_mean, z_log_var, z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Decoder Model.\n",
    "    \n",
    "    Converts z, the encoded FP vector, back into FP vector.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "\n",
    "    def __init__(self, original_dim, latent_dim=2, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.original_dim = original_dim\n",
    "        self.latent_dim   = latent_dim\n",
    "        self.amplify = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Dense(256,  activation='relu' ),\n",
    "                tf.keras.layers.Dense(512, activation='relu' ),\n",
    "            ]\n",
    "        )\n",
    "        self.dense_output = layers.Dense(self.original_dim, activation=\"sigmoid\")\n",
    "\n",
    "        \n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "\n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        x = self.amplify(inputs)\n",
    "        return self.dense_output(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_VAE(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        latent_dim=2,\n",
    "        name=\"vae\",\n",
    "        **kwargs\n",
    "        ):\n",
    "        super(MLP_VAE, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoding = Encoder(original_dim, latent_dim=latent_dim)\n",
    "        self.decoding = Decoder(original_dim)\n",
    "        \n",
    "        # Loss trackers\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.regression_loss_tracker = keras.metrics.MeanSquaredError(name=\"regression_loss\")\n",
    "    \n",
    "    \n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker, \n",
    "                self.reconstruction_loss_tracker, \n",
    "                self.kl_loss_tracker,\n",
    "                self.regression_loss_tracker,\n",
    "                ]\n",
    "        \n",
    "        \n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "  \n",
    "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        z_mean, z_log_var, z = self.encoding(inputs)\n",
    "        reconstructed = self.decoding(z)        \n",
    "        return reconstructed\n",
    "        \n",
    "    \n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "\n",
    "    def regression(self, inputs: tf.Tensor) -> tf.Tensor:\n",
    "        x           = self.encoding.projection(inputs)\n",
    "        reg_mean    = self.encoding.reg_mean(x)\n",
    "        reg_log_var = self.encoding.reg_log_var(x)\n",
    "        reg         = self.encoding.sampling((reg_mean, reg_log_var)) \n",
    "         \n",
    "        return reg\n",
    "    \n",
    "\n",
    "# ==============================================================================\n",
    "# ==============================================================================\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, data) -> Dict[str, float]:\n",
    "        \"\"\"Custome train step function.\n",
    "        \n",
    "        This function contains the LOSS functions and their updates needed to \n",
    "        train the model.\n",
    "        \n",
    "        Returns:\n",
    "            A dictionary containing the updated values of all LOSS functions.\n",
    "            Example:\n",
    "                {loss: 10, reconstruction_loss: 1, \n",
    "                kl_loss: 5, regression_loss: 4}\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        x_train, y_train = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoding(x_train)\n",
    "            y_pred = self.regression(x_train)\n",
    "            \n",
    "            \n",
    "            # Reconstruction Loss \n",
    "            reconstructed       = self.decoding(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_mean(\n",
    "                    keras.losses.binary_crossentropy(x_train, reconstructed, from_logits=True)\n",
    "                            )\n",
    "                        )\n",
    "            \n",
    "            \n",
    "            # KL Loss\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            \n",
    "            \n",
    "            # Regression Loss\n",
    "            regression_loss = keras.losses.MSE(y_pred, y_train)\n",
    "            regression_loss = tf.reduce_mean(regression_loss)\n",
    "            \n",
    "            \n",
    "            # Total Loss\n",
    "            total_loss = reconstruction_loss + kl_loss + regression_loss\n",
    "            \n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        \n",
    "        # Update the Loss Trackers\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.regression_loss_tracker.update_state(y_pred, y_train)\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            \"VAE Loss\":            self.total_loss_tracker.result(),\n",
    "            \"Reconstruction Loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"KL Loss\":             self.kl_loss_tracker.result(),\n",
    "            \"Regression Loss\":     self.regression_loss_tracker.result(),\n",
    "            }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
